#1 - Loading a Google BigQuery table into a DataFrame
# Initializing SparkSession
from pyspark.sql import SparkSession
import base64
spark = SparkSession.builder.master('local').appName('spark-read-from-bigquery').getOrCreate()
# Creating DataFrames
df = spark.read.format('bigquery').option('project','clouderatakedown-lab').option('table','hsi_consumption.sales_open_records').load()
# 2 - Print the Google BigQuery table
df.show()
df.printSchema()
# 3 - Creating or replacing a local temporary view with this DataFrame
df.createOrReplaceTempView("sales_open_records")
query=spark.sql("SELECT `SALES_OPEN_RECORDS`.`itid`, `SALES_OPEN_RECORDS`.`cuid`, `SALES_OPEN_RECORDS`.`product_code`, `SALES_OPEN_RECORDS`.`primary_uom`, `SALES_OPEN_RECORDS`.`invoice_number`, `SALES_OPEN_RECORDS`.`invoice_type_code`, `SALES_OPEN_RECORDS`.`customer_purchase_order_number`, `SALES_OPEN_RECORDS`.`customer_shipto_account_number`, `SALES_OPEN_RECORDS`.`order_billto_account_number`, `SALES_OPEN_RECORDS`.`order_line_number`, `SALES_OPEN_RECORDS`.`sales_order_number`, `SALES_OPEN_RECORDS`.`sales_record_number`, `SALES_OPEN_RECORDS`.`sales_order_type_code`, `SALES_OPEN_RECORDS`.`contract_type_code`, `SALES_OPEN_RECORDS`.`contract_type_class`, `SALES_OPEN_RECORDS`.`contract_type_description`, `SALES_OPEN_RECORDS`.`gp_category_code`, `SALES_OPEN_RECORDS`.`ordered_quantity`, `SALES_OPEN_RECORDS`.`order_source_code`, `SALES_OPEN_RECORDS`.`order_source_description`, `SALES_OPEN_RECORDS`.`order_low_status`, `SALES_OPEN_RECORDS`.`order_low_status_description`, `SALES_OPEN_RECORDS`.`order_line_description`, `SALES_OPEN_RECORDS`.`order_line_status_code`, `SALES_OPEN_RECORDS`.`order_line_status_description`, `SALES_OPEN_RECORDS`.`order_transaction_type_code`, `SALES_OPEN_RECORDS`.`order_last_price_adjustment_type_code`, `SALES_OPEN_RECORDS`.`order_last_price_adjustment_type_description`, `SALES_OPEN_RECORDS`.`order_product_short_02_description`, `SALES_OPEN_RECORDS`.`line_com_message_description`, `SALES_OPEN_RECORDS`.`top_com_message_description`, `SALES_OPEN_RECORDS`.`shipped_quantity`, `SALES_OPEN_RECORDS`.`cancel_quantity`, `SALES_OPEN_RECORDS`.`backorder_quantity`, `SALES_OPEN_RECORDS`.`equipment_order_number_code`, `SALES_OPEN_RECORDS`.`equipment_order_date`, `SALES_OPEN_RECORDS`.`order_creation_date`, `SALES_OPEN_RECORDS`.`cancel_date`, `SALES_OPEN_RECORDS`.`invoice_date`, `SALES_OPEN_RECORDS`.`actual_ship_date`, `SALES_OPEN_RECORDS`.`posting_date`, `SALES_OPEN_RECORDS`.`transaction_date`, `SALES_OPEN_RECORDS`.`requested_delivery_date`, `SALES_OPEN_RECORDS`.`order_line_tax_amount`, `SALES_OPEN_RECORDS`.`extended_sales_amount`, `SALES_OPEN_RECORDS`.`extended_cost`, `SALES_OPEN_RECORDS`.`extended_price`, `SALES_OPEN_RECORDS`.`unit_cost`, `SALES_OPEN_RECORDS`.`unit_price`, (CASE WHEN (`SALES_OPEN_RECORDS`.`order_low_status` < '600') THEN 'Open' WHEN (`SALES_OPEN_RECORDS`.`order_low_status` = '600') THEN 'Shipped' WHEN ((`SALES_OPEN_RECORDS`.`order_low_status` >= '980') AND (NOT ( `SALES_OPEN_RECORDS`.`order_low_status` IN ('986', '988', '990') ))) THEN 'Cancelled' ELSE 'N/A' END) as `order_low_status_custom`, (CASE WHEN (`SALES_OPEN_RECORDS`.`order_line_status_code` < '600') THEN 'Open' WHEN (`SALES_OPEN_RECORDS`.`order_line_status_code` = '600') THEN 'Shipped' WHEN ((`SALES_OPEN_RECORDS`.`order_line_status_code` >= '980') AND (NOT ( `SALES_OPEN_RECORDS`.`order_line_status_code` IN ('986', '988', '990') ))) THEN 'Cancelled' ELSE 'N/A' END) as `order_line_status_code_custom` FROM `sales_open_records` WHERE (`SALES_OPEN_RECORDS`.`sales_order_type_code` != 'CA')")
query.write.format('bigquery').mode('overwrite').option('temporaryGcsBucket','big_query_from_hadoop').option('table','hsi_consumption.dummyspark2').save()
